{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/r5dfhtxd75g2p4pkybxl8jcc0000gn/T/ipykernel_15510/1205883626.py:26: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import Image, display\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import json\n",
    "import datetime\n",
    "from torchvision.transforms import transforms \n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local data_dir \n",
    "# data_dir = \"/Users/Lisa/Desktop/Master Thesis/RoofNet\"\n",
    "data_dir = \"/Users/Lisa/Desktop/Master Thesis/RoofNetXAI/\"\n",
    "model_dir = \"/Users/Lisa/Desktop/Master Thesis/RoofNetXAI/roofnet/saved_models_2.0\"\n",
    "# train/val/test dir\n",
    "data_file_path = data_dir + \"preprocessed_data/train_64_noreroofs.npy\"\n",
    "test_data_file_path = data_dir + \"preprocessed_data/test_64_noreroofs.npy\"\n",
    "val_data_file_path = data_dir + \"preprocessed_data/val_64_noreroofs.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/Lisa/Desktop/Master Thesis/RoofNetXAI/roofnet/utils\")\n",
    "from data import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Done loading data\n",
      "Length 1050\n",
      "Num Roofs 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1050, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Roofnet\n",
    "# from Roofnet.utils.data import ImageDataset\n",
    "from torchvision.transforms import transforms \n",
    "\n",
    "\n",
    "transform_chain = transforms.Compose([\n",
    "                        transforms.ToPILImage(mode='RGB'),\n",
    "                        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.,0.,0,], [1.,1.,1.]),   \n",
    "                        \n",
    "                            ])\n",
    "data = ImageDataset(data_file_path,\n",
    "                    transform=transform_chain)\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size=32, shuffle=True)\n",
    "len(data), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x = next(iter(dataloader))\n",
    "fixed_x = fixed_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# can be skipped\n",
    "# Check to see if data is loaded \n",
    "save_image(fixed_x, 'real_image.png')\n",
    "\n",
    "Image('real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set default parameters\n",
    "image_channels = fixed_x.size(1)\n",
    "img_dim = fixed_x.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import device\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar,beta=1.0):\n",
    "    x = x.to(device)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD*=beta\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/Lisa/Desktop/Master Thesis/RoofNetXAI/roofnet/models\")\n",
    "from vae import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = VAE(img_dim=img_dim,image_channels=image_channels,z_dim=128,device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/Lisa/Desktop/Master Thesis/RoofNetXAI/xai\")\n",
    "from gradcam_vae import GradCAMVAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lisa/anaconda3/envs/roofxai/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/50] Loss: 7748.506 7748.399 0.107\n",
      "Epoch[2/50] Loss: 7324.646 7324.209 0.436\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 1 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     34\u001b[0m gradcam_vae \u001b[39m=\u001b[39m GradCAMVAE(model)\n\u001b[0;32m---> 35\u001b[0m gradcam_maps \u001b[39m=\u001b[39m gradcam_vae\u001b[39m.\u001b[39;49mgenerate_gradcam(images, target_layer \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mencoder\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# Adjust target_layer if needed\u001b[39;00m\n\u001b[1;32m     37\u001b[0m epoch_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(gradcam_maps_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m os\u001b[39m.\u001b[39mmakedirs(epoch_folder, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Master Thesis/RoofNetXAI/xai/gradcam_vae.py:35\u001b[0m, in \u001b[0;36mGradCAMVAE.generate_gradcam\u001b[0;34m(self, input_images, target_layer)\u001b[0m\n\u001b[1;32m     28\u001b[0m     feature_maps \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(feature_maps, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     31\u001b[0m     transforms\u001b[39m.\u001b[39mToPILImage(),\n\u001b[1;32m     32\u001b[0m     transforms\u001b[39m.\u001b[39mResize((input_images\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], input_images\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])),\n\u001b[1;32m     33\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[1;32m     34\u001b[0m ])\n\u001b[0;32m---> 35\u001b[0m resized_feature_maps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([transform(fm) \u001b[39mfor\u001b[39;00m fm \u001b[39min\u001b[39;00m feature_maps])\n\u001b[1;32m     37\u001b[0m \u001b[39m# Compute gradients of the output with respect to the feature maps\u001b[39;00m\n\u001b[1;32m     38\u001b[0m resized_feature_maps\u001b[39m.\u001b[39mrequires_grad_()\n",
      "File \u001b[0;32m~/Desktop/Master Thesis/RoofNetXAI/xai/gradcam_vae.py:35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m     feature_maps \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(feature_maps, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     31\u001b[0m     transforms\u001b[39m.\u001b[39mToPILImage(),\n\u001b[1;32m     32\u001b[0m     transforms\u001b[39m.\u001b[39mResize((input_images\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], input_images\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])),\n\u001b[1;32m     33\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[1;32m     34\u001b[0m ])\n\u001b[0;32m---> 35\u001b[0m resized_feature_maps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([transform(fm) \u001b[39mfor\u001b[39;00m fm \u001b[39min\u001b[39;00m feature_maps])\n\u001b[1;32m     37\u001b[0m \u001b[39m# Compute gradients of the output with respect to the feature maps\u001b[39;00m\n\u001b[1;32m     38\u001b[0m resized_feature_maps\u001b[39m.\u001b[39mrequires_grad_()\n",
      "File \u001b[0;32m~/anaconda3/envs/roofxai/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/roofxai/lib/python3.10/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/anaconda3/envs/roofxai/lib/python3.10/site-packages/torchvision/transforms/functional.py:278\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mndim \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m}:\n\u001b[0;32m--> 278\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39melif\u001b[39;00m pic\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    281\u001b[0m         \u001b[39m# if 2D image, add channel dimension (HWC)\u001b[39;00m\n\u001b[1;32m    282\u001b[0m         pic \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(pic, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 1 dimensions."
     ]
    }
   ],
   "source": [
    "# TRAIN VAE\n",
    "\n",
    "filename = model_dir+'\\\\roofnet_VAE_test_gradcam.pth'\n",
    "epochs = 50\n",
    "bs=32\n",
    "train = True\n",
    "\n",
    "# the path for saving GradCAM maps\n",
    "gradcam_maps_path = \"/Users/Lisa/Desktop/Master Thesis/RoofNetXAI/gradcam_maps\"\n",
    "\n",
    "\n",
    "if train:\n",
    "\n",
    "    start_time = time.time()  # record the start time\n",
    "\n",
    "    epoch = 0\n",
    "    while epoch < epochs:\n",
    "        \n",
    "        for idx in range(100):\n",
    "            images = next(iter(dataloader))\n",
    "            images = images[0]\n",
    "            recon_images, z, mu, logvar, _ = model(images.to(device))\n",
    "            loss, bce, kld = loss_fn(recon_images, images, mu, logvar, beta=5.0)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                    epochs, loss.item()/bs, bce.item()/bs, kld.item()/bs)\n",
    "        epoch += 1\n",
    "        print(to_print)\n",
    "        \n",
    "        if epoch in  [2, 30, 50]:\n",
    "            model.eval()\n",
    "            gradcam_vae = GradCAMVAE(model)\n",
    "            gradcam_maps = gradcam_vae.generate_gradcam(images, target_layer = \"encoder\")  # Adjust target_layer if needed\n",
    "\n",
    "            epoch_folder = os.path.join(gradcam_maps_path, f\"epoch_{epoch}\")\n",
    "            os.makedirs(epoch_folder, exist_ok=True)\n",
    "  \n",
    "            # Overlay GradCAM maps onto the original images and save them\n",
    "            for i in range(images.size(0)):\n",
    "                overlay_image = images[i].clone()\n",
    "                overlay_image[gradcam_maps[i] > 0] = 1.0\n",
    "                save_path = os.path.join(epoch_folder, f\"gradcam_overlay_epoch{epoch}_iter{idx + 1}_image{i + 1}.png\")\n",
    "                save_image(overlay_image, save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(),  filename)\n",
    "    end_time = time.time()  # record the end time\n",
    "    training_time = end_time - start_time  # calculate the training time in seconds\n",
    "    print(\"Training time: {:.2f} seconds\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare real image and reconstruction\n",
    "\n",
    "Note: VAEs are known to generate blurry. The reason is that the latent code is trying to compress as much info as possible and only focus on the meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#64x64\n",
    "import roofnet\n",
    "\n",
    "\n",
    "model = roofnet.models.vae.VAE(img_dim=64,image_channels=image_channels,z_dim=128,device=device).to(device)\n",
    "model.load_state_dict(torch.load(model_dir+'\\\\roofnet_VAE_Beta1.pth'))\n",
    "val_data_file_path = data_dir + \"\\\\val_64_noreroofs.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#255x255\n",
    "model = roofnet.models.vae.VAE(img_dim=255,image_channels=image_channels,z_dim=128,device=device).to(device)\n",
    "model.load_state_dict(torch.load(model_dir+'\\\\roofnet_hard.pth'))\n",
    "val_data_file_path = data_dir + \"\\\\test_easy.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    x=x.to(device)\n",
    "    recon_x,_, _, _ = model(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x = next(iter(dataloader))[0][:1]\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "save_image(compare_x.data.cpu(), 'sample_image.png')\n",
    "display(Image('sample_image.png', width=700, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use validation data to make predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_chain = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.,0.,0,], [1.,1.,1.]),   \n",
    "                        \n",
    "                            ])\n",
    "val_data = ImageDataset(val_data_file_path,\n",
    "                    transform=transform_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=7, shuffle=False)\n",
    "val_dataloader = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x = next(iter(val_dataloader))[0][:1]\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "save_image(compare_x.data.cpu(), 'sample_image.png')\n",
    "display(Image('sample_image.png', width=700, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "out = []\n",
    "latents = []\n",
    "meta = []\n",
    "for i in val_dataloader:\n",
    "    images = i[0]\n",
    "    recon_images,z, _, _ = model(images.to(device))\n",
    "    z = z.detach().cpu().numpy()\n",
    "    latents.append(z)\n",
    "    meta.append([i[2]['address'][0],int(i[2]['transition_year'][0].cpu().numpy())])\n",
    "    d = int(np.argmax([np.linalg.norm(zi-zj) for zi,zj in zip(z[1:],z[:-1])]))+2013\n",
    "    out.append([i[2]['address'][0],d,int(i[2]['transition_year'][0].cpu().numpy())])\n",
    "out = np.array(out)\n",
    "latents = np.array(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = 0\n",
    "l = 0\n",
    "for i in out:\n",
    "    if i[2] != 0:\n",
    "        l += 1\n",
    "        if i[1]==i[2]:\n",
    "            hold += 1\n",
    "print(hold/l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at just the argmax of distance between latents does not produce a great accuracy. There are clearly many images where the largest distance between latents is a terrible metric for reroof. The verticle bar represents the reroof date, if it is missing there is no reroof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(latents)):\n",
    "    diff = []\n",
    "    for i in range(6):\n",
    "        diff.append(np.linalg.norm(latents[j][i]-latents[j][i+1]))\n",
    "    plt.plot(diff)\n",
    "    plt.title(meta[j][0])\n",
    "    if meta[j][1] != 0:\n",
    "        plt.axvline(int(meta[j][1])-2013)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build binary classifier on latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs: latent image 1, latent image 2, classification model, and prob threshold\n",
    "# Returns binary classification, 1 for reroof, 0 for none.\n",
    "def classify_image_pair(latent_1,latent_2,model,prob_threshold=0.5):\n",
    "    v_1 = np.hstack((latent_1,latent_2))\n",
    "    v_2 = np.hstack((latent_2,latent_1))\n",
    "    model.eval()\n",
    "    p_1 = net(torch.tensor(v_1).to(device))\n",
    "    p_2 = net(torch.tensor(v_2).to(device))\n",
    "    prob = max([p_1.item(),p_2.item()])\n",
    "    return prob >= prob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: Latents for a building, meta for building, model, and threshold\n",
    "# Output: [Address, predicted_transition, actual_transition]\n",
    "# Returns transition year based on first transition detected\n",
    "def gen_predictions(latents,meta,model,prob_threshold=0.5):\n",
    "    out = []\n",
    "    for i in range(len(latents)):\n",
    "        hold = []\n",
    "        hold.append(str(meta[i][0]))\n",
    "        trans_year = 0\n",
    "        for j in range(len(latents[i])-1):\n",
    "            if classify_image_pair(latents[i][j],latents[i][j+1],model,prob_threshold=prob_threshold):\n",
    "                trans_year = j + 2013\n",
    "                break\n",
    "        hold.append(trans_year)\n",
    "        hold.append(int(meta[i][1]))\n",
    "        out.append(hold)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: Latents for a building, meta for building, model, and threshold\n",
    "# Output: [Address, predicted_transition, actual_transition]\n",
    "# Returns transition year based on highest probability\n",
    "def get_max_prob(latents,meta,model,prob_threshold=0.5):\n",
    "    out = []\n",
    "    for i in range(len(latents)):\n",
    "        hold = []\n",
    "        hold.append(str(meta[i][0]))\n",
    "        trans_year = 0\n",
    "        temp = []\n",
    "        for j in range(len(latents[i])-1):\n",
    "            latent_1 = latents[i][j]\n",
    "            latent_2 = latents[i][j+1]\n",
    "            v_1 = np.hstack((latent_1,latent_2))\n",
    "            v_2 = np.hstack((latent_2,latent_1))\n",
    "            model.eval()\n",
    "            p_1 = net(torch.tensor(v_1).to(device))\n",
    "            p_2 = net(torch.tensor(v_2).to(device))\n",
    "            prob = max([p_1.item(),p_2.item()])\n",
    "            temp.append(prob)\n",
    "        index = np.argmax(temp)\n",
    "        if temp[index]>=prob_threshold:\n",
    "            trans_year = 2013 + index\n",
    "        hold.append(trans_year)\n",
    "        hold.append(int(meta[i][1]))\n",
    "        out.append(hold)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates accuracy metrics for detecting reroof and predicting reroof date\n",
    "def gen_metrics(acc_metric, latents, meta, model, threshold=0.5):\n",
    "    no_reroof_pred = []\n",
    "    reroof_pred = []\n",
    "    hold = acc_metric(latents,meta,net,threshold)\n",
    "    print(\"Overall accuracy: {:0.3f}\".format(np.mean(hold[:,1]==hold[:,2])))\n",
    "    for i in hold:\n",
    "        if int(i[2])==0:\n",
    "            no_reroof_pred.append(int(int(i[1]) == 0))\n",
    "        else:\n",
    "            reroof_pred.append(int(i[1]==i[2]))\n",
    "    print(\"No reroof prediction accuracy: {:0.3f}\".format(np.mean(no_reroof_pred)))\n",
    "    print(\"Reroof prediction accuracy: {:0.3f}\".format(np.mean(reroof_pred)))\n",
    "    return hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs: file path for data\n",
    "# Outputs: Dataloader for training\n",
    "\n",
    "def gen_dataloader(data_file_path):\n",
    "    transform_chain = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.,0.,0,], [1.,1.,1.]),   \n",
    "                        \n",
    "                            ])\n",
    "    data = ImageDataset(data_file_path,\n",
    "                        transform=transform_chain)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=7, shuffle=False)\n",
    "    dataloader = iter(dataloader)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: dataloader, latent generating model\n",
    "# Outputs: Latents generate by model, meta data for buildings\n",
    "def gen_latents(dataloader, model):\n",
    "    latents = []\n",
    "    meta = []\n",
    "    for i in dataloader:\n",
    "        images = i[0]\n",
    "        _, z, _, _ = model(images.to(device))\n",
    "        z = z.detach().cpu().numpy()\n",
    "        latents.append(z)\n",
    "        meta.append([i[2]['address'][0],int(i[2]['transition_year'][0].cpu().numpy())])\n",
    "    latents = np.array(latents)\n",
    "    return latents, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs: Latents of data, meta for building\n",
    "# Output: All possible pairs of latent images and their label 1:reroof, 0:no reroof\n",
    "def gen_binary_data(latents, meta):\n",
    "    data_hold = []\n",
    "    label_hold = []\n",
    "    for i in range(len(latents)):\n",
    "        for j in range(len(latents[i])):\n",
    "            for k in range(len(latents[i])):\n",
    "                data_hold.append(np.hstack((latents[i][j],latents[i][k])))\n",
    "                year_j = 2012+j < meta[i][1]\n",
    "                year_k = 2012+k < meta[i][1]\n",
    "                label_hold.append(float(year_j != year_k))\n",
    "    data_hold = np.array(data_hold)\n",
    "    label_hold = np.array(label_hold)\n",
    "    \n",
    "    return data_hold, label_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_train_epoch(model, opt, criterion, data, labels, data_loader, val_data, val_labels, val_data_loader, best_acc, logit=False):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    model.train()\n",
    "    loss_hold = []\n",
    "    \n",
    "    for i in iter(data_loader):\n",
    "        batch_size = len(i)\n",
    "        x_batch = data[i]\n",
    "        y_batch = labels[i]\n",
    "        \n",
    "        x_batch = Variable(torch.from_numpy(x_batch))\n",
    "        y_batch = torch.tensor(y_batch, dtype=torch.float, device=device)\n",
    "        y_batch = y_batch.view(batch_size,-1)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_batch.to(model.device))\n",
    "        loss = criterion(y_hat,y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        loss_hold.append(loss.item()/batch_size)\n",
    "    print(\"Epoch training loss:{:.3f}\".format(np.mean(loss_hold)))\n",
    "    \n",
    "    #validate\n",
    "    model.eval()\n",
    "    pred_acc = []\n",
    "    for i in iter(val_data_loader):\n",
    "        batch_size = len(i)\n",
    "        x_batch = val_data[i]\n",
    "        y_batch = val_labels[i]\n",
    "        \n",
    "        x_batch = Variable(torch.from_numpy(x_batch))\n",
    "        \n",
    "        if logit:\n",
    "            y_logits = model(x_batch.to(model.device))\n",
    "            s = nn.Sigmoid()\n",
    "            y_hat = s(y_logits)>0.5\n",
    "        else:\n",
    "            y_hat = model(x_batch.to(model.device))>0.5\n",
    "        \n",
    "        pred_acc.append(np.mean(y_hat.cpu().numpy() == y_batch.reshape(batch_size,-1)))\n",
    "        \n",
    "    pred_acc = np.mean(pred_acc)\n",
    "    print(\"Epoch validation accuracy: {:.3f}%\".format(pred_acc))\n",
    "    \n",
    "    if pred_acc >= best_acc:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = pred_acc\n",
    "    \n",
    "    net.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return net, best_acc\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = roofnet.models.vae.VAE(img_dim=64,image_channels=image_channels,z_dim=128,device=device).to(device)\n",
    "model.load_state_dict(torch.load(model_dir+'\\\\roofnet_VAE_Beta5.pth'))\n",
    "\n",
    "data_file_path = data_dir + \"\\\\train_64_noreroofs.npy\"\n",
    "\n",
    "binary_dataloader = gen_dataloader(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_file_path = data_dir + \"\\\\val_64_noreroofs.npy\"\n",
    "\n",
    "binary_val_dataloader = gen_dataloader(val_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = data_dir + \"\\\\test_64_noreroofs.npy\"\n",
    "\n",
    "binary_test_dataloader = gen_dataloader(test_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latents, meta = gen_latents(binary_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_latents, val_meta = gen_latents(binary_val_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_latents, test_meta = gen_latents(binary_test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_hold, label_hold = gen_binary_data(latents, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_hold, val_label = gen_binary_data(val_latents, val_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hold, test_label = gen_binary_data(test_latents, test_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, zdim = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2*zdim, 2*zdim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2*zdim, zdim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(zdim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(self.device)\n",
    "              \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.network(x)\n",
    "        return y\n",
    "    \n",
    "net = Net()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_loader = torch.utils.data.DataLoader(np.arange(len(data_hold)),batch_size=32,shuffle=True)\n",
    "val_index_loader = torch.utils.data.DataLoader(np.arange(len(val_hold)),batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_acc = 0\n",
    "for e in range(num_epochs):\n",
    "    print('Running epoch {}/{}'.format(e,num_epochs))\n",
    "    net, best_acc = binary_train_epoch(net, opt, criterion, data_hold, label_hold, index_loader, val_hold, val_label, val_index_loader, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliest transition method prediction:\n",
    "hold = gen_metrics(gen_predictions, val_latents, val_meta, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(model_dir+'\\\\roofnet_binary_class_hard_easyval.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), model_dir+'\\\\roofnet_binary_class_b5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = gen_metrics(get_max_prob, val_latents, val_meta, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = gen_metrics(get_max_prob, test_latents, test_meta, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = {'site_address': hold[:,0], 'transition_true': hold[:,2], 'transition_predicted': hold[:,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(out).to_csv('Argmax(Prob) Beta5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Hyperparameter tuning on prediction threshold, best to leave at 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = [Address, predicted_transition, actual_transition]\n",
    "for i in range(21):\n",
    "    pt = 0.4+0.01*i\n",
    "    print(pt)\n",
    "    no_reroof_pred = []\n",
    "    reroof_pred = []\n",
    "    hold = get_max_prob(val_latents,val_meta,net,prob_threshold=pt)\n",
    "    print(\"Overall accuracy: {:0.3f}\".format(np.mean(hold[:,1]==hold[:,2])))\n",
    "    for i in hold:\n",
    "        if int(i[2])==0:\n",
    "            no_reroof_pred.append(int(int(i[1]) == 0))\n",
    "        else:\n",
    "            reroof_pred.append(int(i[1]==i[2]))\n",
    "    print(\"No reroof prediction accuracy: {:0.3f}\".format(np.mean(no_reroof_pred)))\n",
    "    print(\"Reroof prediction accuracy: {:0.3f}\".format(np.mean(reroof_pred)))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = [Address, predicted_transition, actual_transition]\n",
    "for i in range(11):\n",
    "    pt = 0.3+0.01*i\n",
    "    print(pt)\n",
    "    no_reroof_pred = []\n",
    "    reroof_pred = []\n",
    "    hold = get_max_prob(latents,meta,net,prob_threshold=pt)\n",
    "    print(\"Overall accuracy: {:0.3f}\".format(np.mean(hold[:,1]==hold[:,2])))\n",
    "    for i in hold:\n",
    "        if int(i[2])==0:\n",
    "            no_reroof_pred.append(int(int(i[1]) == 0))\n",
    "        else:\n",
    "            reroof_pred.append(int(i[1]==i[2]))\n",
    "    print(\"No reroof prediction accuracy: {:0.3f}\".format(np.mean(no_reroof_pred)))\n",
    "    print(\"Reroof prediction accuracy: {:0.3f}\".format(np.mean(reroof_pred)))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = [Address, predicted_transition, actual_transition]\n",
    "for i in range(21):\n",
    "    pt = 0.1+0.01*i\n",
    "    print(pt)\n",
    "    no_reroof_pred = []\n",
    "    reroof_pred = []\n",
    "    hold = get_max_prob(latents,meta,net,prob_threshold=pt)\n",
    "    print(\"Overall accuracy: {:0.3f}\".format(np.mean(hold[:,1]==hold[:,2])))\n",
    "    for i in hold:\n",
    "        if int(i[2])==0:\n",
    "            no_reroof_pred.append(int(int(i[1]) == 0))\n",
    "        else:\n",
    "            reroof_pred.append(int(i[1]==i[2]))\n",
    "    print(\"No reroof prediction accuracy: {:0.3f}\".format(np.mean(no_reroof_pred)))\n",
    "    print(\"Reroof prediction accuracy: {:0.3f}\".format(np.mean(reroof_pred)))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#out = [Address, predicted_transition, actual_transition]\n",
    "for i in range(21):\n",
    "    pt = 0.4+0.01*i\n",
    "    print(pt)\n",
    "    no_reroof_pred = []\n",
    "    reroof_pred = []\n",
    "    hold = get_max_prob(latents,meta,net,prob_threshold=pt)\n",
    "    print(\"Overall accuracy: {:0.3f}\".format(np.mean(hold[:,1]==hold[:,2])))\n",
    "    for i in hold:\n",
    "        if int(i[2])==0:\n",
    "            no_reroof_pred.append(int(int(i[1]) == 0))\n",
    "        else:\n",
    "            reroof_pred.append(int(i[1]==i[2]))\n",
    "    print(\"No reroof prediction accuracy: {:0.3f}\".format(np.mean(no_reroof_pred)))\n",
    "    print(\"Reroof prediction accuracy: {:0.3f}\".format(np.mean(reroof_pred)))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roofxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8894df8dd8fdbd6c91ecf09b4a5d40bec4e42bef395be83350036acab2607e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
